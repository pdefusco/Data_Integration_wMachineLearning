{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splink data linking demo (link only)\n",
    "\n",
    "In this demo we link two small datasets.  \n",
    "\n",
    "We assume we have a list of people in one table who we want to find in a larger table.  It is assumed that due to transcription or other errors, there will often not be an exact match.\n",
    "\n",
    "The larger table contains duplicates, but in this notebook we use the `link_only` setting, so `splink` makes no attempt to deduplicate these records.    Note it is possible to simultaneously link and dedupe using the `link_and_dedupe` setting.\n",
    "\n",
    "**Important** Where deduplication is not required, `link_only` can provide an important performance boost by dramatically reducing the number of records which need to be compared.\n",
    "\n",
    "For example, if you wanted to link 10 records to 1,000, then the maximum number of comparisons that need to be made (i.e. with no blocking rules) is 10,000.  If you need to dedupe as well, that number would be n(n-1)/2 = 509,545.\n",
    "\n",
    "I print the output at each stage using `spark_dataframe.show()`.  This is for instructional purposes only - it degrades performance and shouldn't be used in a production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Imports and setup\n",
    "\n",
    "The following is just boilerplate code that sets up the Spark session and sets some other non-essential configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting splink\n",
      "  Downloading splink-1.0.5-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typeguard<3.0.0,>=2.10.0 in ./.local/lib/python3.6/site-packages (from splink) (2.11.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema<4.0,>=3.2 in /usr/local/lib/python3.6/site-packages (from splink) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from jsonschema<4.0,>=3.2->splink) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.11.0 in /usr/local/lib/python3.6/site-packages (from jsonschema<4.0,>=3.2->splink) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/site-packages (from jsonschema<4.0,>=3.2->splink) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/site-packages (from jsonschema<4.0,>=3.2->splink) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/site-packages (from jsonschema<4.0,>=3.2->splink) (40.6.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema<4.0,>=3.2->splink) (3.1.0)\n",
      "Installing collected packages: splink\n",
      "  Attempting uninstall: splink\n",
      "    Found existing installation: splink 1.0.4\n",
      "    Uninstalling splink-1.0.4:\n",
      "      Successfully uninstalled splink-1.0.4\n",
      "Successfully installed splink-1.0.5\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade splink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct Cloud Storage URL is: s3a://demo-aws-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "\n",
    "#Extracting the correct URL from hive-site.xml\n",
    "tree = ET.parse('/etc/hadoop/conf/hive-site.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for prop in root.findall('property'):\n",
    "    if prop.find('name').text == \"hive.metastore.warehouse.dir\":\n",
    "        storage = prop.find('value').text.split(\"/\")[0] + \"//\" + prop.find('value').text.split(\"/\")[2]\n",
    "\n",
    "print(\"The correct Cloud Storage URL is: {}\".format(storage))\n",
    "\n",
    "os.environ['STORAGE'] = storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=SparkConf()\n",
    "\n",
    "# Load in a jar that provides extended string comparison functions such as Jaro Winkler.\n",
    "# Splink\n",
    "#     conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#     conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\n",
    "\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc.setCheckpointDir(\"temp_graphframes/\")\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Entity Resolution with Lineage\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ['STORAGE'])\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .config(\"spark.jars\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register UDFs\n",
    "from pyspark.sql import types\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim', 'uk.gov.moj.dash.linkage.JaroWinklerSimilarity', types.DoubleType())\n",
    "spark.udf.registerJavaFunction('Dmetaphone', 'uk.gov.moj.dash.linkage.DoubleMetaphone', types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://100.100.186.93:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5.7.2.2.0-244</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.20.0.1:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Entity Resolution with Lineage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc2780b5160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig()  # Means logs will print in Jupyter Lab\n",
    "\n",
    "# Set to DEBUG if you want splink to log the SQL statements it's executing under the hood\n",
    "logging.getLogger(\"splink\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the data\n",
    "\n",
    "The `l` and `r` stand for 'left' and 'right.  It doesn't matter which of the two datasets you choose as the left, performance and results will be the same.\n",
    "\n",
    "⚠️ Note that `splink` makes the following assumptions about your data:\n",
    "\n",
    "-  There is a field containing a unique record identifier in each dataset\n",
    "-  The two datasets being linked have common column names - e.g. date of birth is represented in both datasets in a field of the same name.   In many cases, this means that the user needs to rename columns prior to using `splink`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of rows in `df_1` is 181\n",
      "+---------+----------+-------+----------+------------+--------------------+-----+--------------+\n",
      "|unique_id|first_name|surname|       dob|        city|               email|group|source_dataset|\n",
      "+---------+----------+-------+----------+------------+--------------------+-----+--------------+\n",
      "|        0|    Julia |   null|2015-10-29|      London| hannah88@powers.com|    0|          df_1|\n",
      "|        4|      oNah| Watson|2008-03-23|      Bolton|matthew78@ballard...|    1|          df_1|\n",
      "|       13|    Molly |   Bell|2002-01-05|Peterborough|                null|    2|          df_1|\n",
      "|       15| Alexander|Amelia |1983-05-19|     Glasgow|ic-mpbell@alleale...|    3|          df_1|\n",
      "|       20|    Ol vri|ynnollC|1972-03-08|    Plymouth|derekwilliams@nor...|    4|          df_1|\n",
      "+---------+----------+-------+----------+------------+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The count of rows in `df_2` is 819\n",
      "+---------+----------+-------+----------+------+--------------------+-----+--------------+\n",
      "|unique_id|first_name|surname|       dob|  city|               email|group|source_dataset|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+--------------+\n",
      "|        1|    Julia | Taylor|2015-07-31|London| hannah88@powers.com|    0|          df_2|\n",
      "|        2|    Julia | Taylor|2016-01-27|London| hannah88@powers.com|    0|          df_2|\n",
      "|        3|    Julia | Taylor|2015-10-29|  null|  hannah88opowersc@m|    0|          df_2|\n",
      "|        5|     Noah | Watson|2008-03-23|Bolton|matthew78@ballard...|    1|          df_2|\n",
      "|        6|    Watson|  Noah |2008-03-23|  null|matthew78@ballard...|    1|          df_2|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "df_1 = spark.read.parquet(\"data/fake_df_l.parquet\")\n",
    "df_1 = df_1.withColumn(\"source_dataset\", lit(\"df_1\"))\n",
    "df_2 = spark.read.parquet(\"data/fake_df_r.parquet\")\n",
    "df_2 = df_2.withColumn(\"source_dataset\", lit(\"df_2\"))\n",
    "print(f\"The count of rows in `df_1` is {df_1.count()}\")\n",
    "df_1.show(5)\n",
    "print(f\"The count of rows in `df_2` is {df_2.count()}\")\n",
    "df_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(unique_id,LongType,true),StructField(first_name,StringType,true),StructField(surname,StringType,true),StructField(dob,StringType,true),StructField(city,StringType,true),StructField(email,StringType,true),StructField(group,LongType,true)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Configure splink using the `settings` object\n",
    "\n",
    "Most of `splink` configuration options are stored in a settings dictionary.  This dictionary allows significant customisation, and can therefore get quite complex.  \n",
    "\n",
    "💥 We provide an tool for helping to author valid settings dictionaries, which includes tooltips and autocomplete, which you can find [here](http://robinlinacre.com/splink_settings_editor/).\n",
    "\n",
    "Customisation overrides default values built into splink.  For the purposes of this demo, we will specify a simple settings dictionary, which means we will be relying on these sensible defaults.\n",
    "\n",
    "To help with authoring and validation of the settings dictionary, we have written a [json schema](https://json-schema.org/), which can be found [here](https://github.com/moj-analytical-services/splink/blob/master/splink/files/settings_jsonschema.json).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparison expression allows for the case where a first name and surname have been inverted \n",
    "sql_case_expression = \"\"\"\n",
    "CASE \n",
    "WHEN first_name_l = first_name_r AND surname_l = surname_r THEN 4 \n",
    "WHEN first_name_l = surname_r AND surname_l = first_name_r THEN 3\n",
    "WHEN first_name_l = first_name_r THEN 2\n",
    "WHEN surname_l = surname_r THEN 1\n",
    "ELSE 0 \n",
    "END\n",
    "\"\"\"\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"link_only\", \n",
    "    \"max_iterations\": 20,\n",
    "    \"blocking_rules\": [\n",
    "    ],\n",
    "    \"comparison_columns\": [\n",
    "       {\n",
    "            \"custom_name\": \"name_inversion\",\n",
    "            \"custom_columns_used\": [\"first_name\", \"surname\"],\n",
    "            \"case_expression\": sql_case_expression,\n",
    "            \"num_levels\": 5\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"email\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        }\n",
    "    ],\n",
    "    \"additional_columns_to_retain\": [\"group\"]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this setting dictionary says:\n",
    "\n",
    "- We are performing a data linking task (the other options are `dedupe_only`, or `link_and_dedupe`)\n",
    "- Rather than generate all possible comparisons (the cartesian product of the input datasets), we are going restrict record comparisons to those generated by at least one of the rules in the specified array\n",
    "- When comparing records, we will use information from the `first_name`, `surname`, `dob`, `city` and `email` columns to compute a match score.\n",
    "- For `first_name` and `surname`, string comparisons will have three levels:\n",
    "    - Level 2: Strings are (almost) exactly the same\n",
    "    - Level 1: Strings are similar \n",
    "    - Level 0: No match\n",
    "- We will make adjustments for term frequencies on the `first_name` and `surname` columns\n",
    "- We will retain the `group` column in the results even though this is not used as part of comparisons.  This is a labelled dataset and `group` contains the true match - i.e. where group matches, the records pertain to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Two Datasets as Spark Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_table_left')\n",
    "df_r.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_table_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+----------+---------------+--------------------+-----+\n",
      "|unique_id|first_name|  surname|       dob|           city|               email|group|\n",
      "+---------+----------+---------+----------+---------------+--------------------+-----+\n",
      "|        0|    Julia |     null|2015-10-29|         London| hannah88@powers.com|    0|\n",
      "|        4|      oNah|   Watson|2008-03-23|         Bolton|matthew78@ballard...|    1|\n",
      "|       13|    Molly |     Bell|2002-01-05|   Peterborough|                null|    2|\n",
      "|       15| Alexander|  Amelia |1983-05-19|        Glasgow|ic-mpbell@alleale...|    3|\n",
      "|       20|    Ol vri|  ynnollC|1972-03-08|       Plymouth|derekwilliams@nor...|    4|\n",
      "|       23|      null| Thompson|1996-03-22|          Leeds|jefferyduke@brown...|    5|\n",
      "|       27|  Matilda |    Hsrir|1983-04-30|         London| patrcio47@davis.cam|    6|\n",
      "|       32|    Baxter|    Aria |1992-09-07|         London|christineshepherd...|    7|\n",
      "|       37|    Wilson| Charlie |1998-09-15|         London|samantha81@henry.com|    8|\n",
      "|       38|   Olivia |  Andrews|2009-01-23|           null|hesterkurt@taylor...|    9|\n",
      "|       43|    Chloe |Macdonald|2015-02-04|           null| jennifer25@hill.biz|   10|\n",
      "|       44|   Millie |    Harrs|1996-07-27|         London|    lynn04@jones.com|   11|\n",
      "|       49| Theodore |     null|1982-09-10|          Leeds|markstewart@gross...|   12|\n",
      "|       58|   Elijah |     null|1970-06-01|     Sunderland|barbaraolson@smit...|   13|\n",
      "|       63|     Maya |    Jones|2010-08-05|     Sunderland|     erin01@york.com|   14|\n",
      "|       69| Isabella |    Coepo|1987-11-18|Stockton-Tn-oee|imoore@turner-gon...|   15|\n",
      "|       79|      Carr|   Ethan |2013-01-21|         London|stacyball@medina.biz|   16|\n",
      "|       88|      Eva |   Harris|1971-01-29|     Birmingham|vwilson@reyes-tor...|   17|\n",
      "|       89|   Chirla |   Taylor|2006-06-28|         London|  mbrooks@booker.com|   18|\n",
      "|       91|    Lilly |    Jones|2003-12-30|           null|  samue91rcu@ry.info|   19|\n",
      "+---------+----------+---------+----------+---------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_l.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Estimate match scores using the Expectation Maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.6/site-packages/splink/default_settings.py:200: UserWarning: You have not specified any blocking rules, meaning all comparisons between the input dataset(s) will be generated and blocking will not be used.For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared.\n",
      "  col_settings[key] = default\n",
      "INFO:splink.iterate:Iteration 0 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.40568520724773405 for key name_inversion, level 4\n",
      "INFO:splink.iterate:Iteration 1 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.06933289766311646 for key email, level 1\n",
      "INFO:splink.iterate:Iteration 2 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.02503591775894165 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 3 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.009511321783065796 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 4 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.004227638244628906 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 5 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0022344589233398438 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 6 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.001312553882598877 for key dob, level 1\n",
      "INFO:splink.iterate:Iteration 7 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0008212625980377197 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 8 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0005371570587158203 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 9 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0003641173243522644 for key city, level 0\n",
      "INFO:splink.iterate:Iteration 10 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0002571418881416321 for key city, level 0\n",
      "INFO:splink.iterate:Iteration 11 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0001854151487350464 for key city, level 0\n",
      "INFO:splink.iterate:Iteration 12 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0001360774040222168 for key city, level 0\n",
      "INFO:splink.iterate:Iteration 13 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0001013725996017456 for key city, level 0\n",
      "INFO:splink.iterate:Iteration 14 complete\n",
      "INFO:splink.model:The maximum change in parameters was 7.649511098861694e-05 for key city, level 0\n",
      "INFO:splink.iterate:EM algorithm has converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[match_probability: double, source_dataset_l: string, unique_id_l: bigint, source_dataset_r: string, unique_id_r: bigint, first_name_l: string, first_name_r: string, surname_l: string, surname_r: string, gamma_name_inversion: int, city_l: string, city_r: string, gamma_city: int, email_l: string, email_r: string, gamma_email: int, dob_l: string, dob_r: string, gamma_dob: int, group_l: bigint, group_r: bigint]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from splink import Splink\n",
    "\n",
    "linker = Splink(settings, [df_1, df_2], spark)\n",
    "df_e = linker.get_scored_comparisons()\n",
    "\n",
    "# Later, we will make term frequency adjustments.  \n",
    "# Persist caches these results in memory, preventing them having to be recomputed when we make these adjustments.\n",
    "df_e.persist()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect main dataframe that contains the match scores\n",
    "df_e.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` property of the `linker` is an object that contains a lot of diagnostic information about how the match probability was computed.  The following cells demonstrate some of its functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative representation of the parameters displays them in terms of the effect different values in the comparison vectors have on the match probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.bayes_factor_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If charts aren't displaying correctly in your notebook, you can write them to a file (by default splink_charts.html)\n",
    "params.all_charts_write_html_file(\"splink_charts.html\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a report which explains how the match probability was computed for an individual comparison row.  \n",
    "\n",
    "Note that you need to convert the row to a dictionary for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.intuition import intuition_report\n",
    "row_dict = df_e.toPandas().sample(1).to_dict(orient=\"records\")[0]\n",
    "print(intuition_report(row_dict, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.diagnostics import splink_score_histogram\n",
    "from pyspark.sql.functions import expr \n",
    "splink_score_histogram(df_e.filter(expr('match_probability > 0.001')), spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Custom Atlas Type (Process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to instantiate the connection to Atlas in CDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atlasclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint, Username and Passoword are stored as CML project variables and passed dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlasclient.client import Atlas\n",
    "client = Atlas(os.environ[\"atlas_endpoint\"], port='', username=os.environ[\"atlas_username\"], password=os.environ[\"atlas_password\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Client connection is working by querying a random Atlas entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid = \"c845eb62-d85d-4591-8abe-0c31449cdd95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.entity['attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have successfully established the connection. Next we can create a custom Atlas type (process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"EM_algorithm_linkage\",\n",
    "        \"description\":\"custom_type_for_Entity_Resolution\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now register the new type with Atlas. For more on the Atlas type model, please visit this page: https://docs.cloudera.com/runtime/7.2.7/cdp-governance-overview/topics/atlas-metadata-model-overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Has already run once so will not run again\n",
    "#client.typedefs.create(data=typedef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Instantiate the EM algorithm in Atlas along with lineage reflecting our Linkage Job above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we need to pass the Atlas guid for the two datasets we compared above as they were registered in Atlas when they were stored as a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving GUID's for the three tables via Atlas Client - search by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'typeName': 'hive_table', 'attrName': 'data', 'attrValue': 'provider', 'offset': '1', 'limit':'10'}\n",
    "#search_results = client.search_basic(**params)\n",
    "#for s in search_results:\n",
    "#    for e in s.entities:\n",
    "#        print(e.guid)\n",
    "#        print(e.attributes)\n",
    "#        print(e.attributes.values)\n",
    "#        print(e.typeName)\n",
    "#        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit':'10'}\n",
    "#search_results = client.search_attribute(**params)\n",
    "#for s in search_results:\n",
    "#    for e in s.entities:\n",
    "#        print(e.guid)\n",
    "#        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s in search_results:\n",
    "#    print(s.entities.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit': '100'}\n",
    "#search_results = client.search_basic.create(data=data)\n",
    "#for e in search_results.entities:\n",
    "#    print(e.guid)\n",
    "#    print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574333\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"EM_algorithm_linkage\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"EM Record Linkage\",\n",
    "      \"name\":\"EM Record Linkage\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"aa955089-5a11-46d9-9dbf-2f6b75f4d65b\", \"typeName\":\"hive_table\"},\n",
    "               {\"guid\": \"43d788ce-4af4-4253-af0b-465ea45c1b93\", \"typeName\":\"hive_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"ac1bdcb3-73c8-4198-a8e6-0aa104c606bb\", \"type_name\":\"hive_table\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.entity_post.create(data=process_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Navigate to Atlas (SDX) and browse for the \"EM_algorithm_linkage\" entity. Expand the lineage tab and the source and target datasets will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ER_atlas_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can optionally remove the EM Algorithm instance from Atlas via the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(\"44848fe5-6950-4a73-a89c-9775b736b4c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.entity['attributes'][\"owner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have completed our introduction to Splink and the Atlas Client. \n",
    "## Next we will simulate a real world Application with CML Jobs and COD (Cloudera Operational Database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
